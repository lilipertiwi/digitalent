{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk import word_tokenize\n",
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('ate', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('mouse', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('after', 'IN'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and give POS Tags\n",
    "new=\"The big cat ate the little mouse who was after fresh cheese\"\n",
    "new_tokens=nltk.pos_tag(word_tokenize(new))\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S I/PRP watch/VBP (NP The/DT Lion/NNP King/NNP) (NP tonight/NN))\n"
     ]
    }
   ],
   "source": [
    "new = \"I watch The Lion King tonight\"\n",
    "new_lion = nltk.pos_tag(word_tokenize(new))\n",
    "\n",
    "grammar_lion=r\"NP: {(<DT>?<NNP>*) | (<DT>?<NNP>*<NN>)}\"\n",
    "chunk_parser=nltk.RegexpParser(grammar_lion)\n",
    "\n",
    "chunk_result=chunk_parser.parse(new_lion)\n",
    "print(chunk_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subject–Verb\n",
    "Subject–Verb–Object\n",
    "Subject–Verb–Adjective\n",
    "Subject–Verb–Adverb\n",
    "Subject–Verb–Noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('reads', 'VBZ'),\n",
       " ('many', 'JJ'),\n",
       " ('books', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('library', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"He reads many books in the library.\"\n",
    "sent1_tokens=nltk.pos_tag(word_tokenize(sent1))\n",
    "sent1_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  He/PRP\n",
      "  reads/VBZ\n",
      "  many/JJ\n",
      "  books/NNS\n",
      "  (NP in/IN the/DT library/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar1=r\"NP: {<IN>?<DT>?<NN>}\"\n",
    "chunk_parser=nltk.RegexpParser(grammar1)\n",
    "\n",
    "chunk1_result=chunk_parser.parse(sent1_tokens)\n",
    "print(chunk1_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP He/PRP reads/VBZ)\n",
      "  many/JJ\n",
      "  books/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  library/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar2=r\" PP: {<PRP><VBZ>}  \"\n",
    "chunk_parser=nltk.RegexpParser(grammar2)\n",
    "\n",
    "chunk1_result=chunk_parser.parse(sent1_tokens)\n",
    "print(chunk1_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('laptop', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('already', 'RB'),\n",
       " ('crashed', 'VBN'),\n",
       " ('twice', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2 = \"This new laptop has already crashed twice.\"\n",
    "sent2_tokens=nltk.pos_tag(word_tokenize(sent2))\n",
    "sent2_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  This/DT\n",
      "  new/JJ\n",
      "  (NP laptop/NN has/VBZ already/RB crashed/VBN)\n",
      "  twice/RB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar3=r\" NP: {<NN><.*>*<VBN>}\"\n",
    "chunk_parser=nltk.RegexpParser(grammar3)\n",
    "\n",
    "chunk3_result=chunk_parser.parse(sent2_tokens)\n",
    "print(chunk3_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lara', 'NNP'),\n",
       " ('eats', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('delicious', 'JJ'),\n",
       " ('hamburger', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('Sophie', 'NNP'),\n",
       " ('drinks', 'NNS'),\n",
       " ('water', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent3 = \"Lara eats a delicious hamburger and Sophie drinks water.\"\n",
    "sent3_tokens=nltk.pos_tag(word_tokenize(sent3))\n",
    "sent3_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Lara/NNP eats/VBZ)\n",
      "  a/DT\n",
      "  delicious/JJ\n",
      "  hamburger/NN\n",
      "  and/CC\n",
      "  (NP Sophie/NNP drinks/NNS)\n",
      "  water/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar4=r\" NP: {<NNP><.*>}\"\n",
    "chunk_parser=nltk.RegexpParser(grammar4)\n",
    "\n",
    "chunk4_result=chunk_parser.parse(sent3_tokens)\n",
    "print(chunk4_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mom', 'NN'), ('cooks', 'VBZ'), ('chicken', 'VBN'), ('soup', 'NN'), ('for', 'IN'), ('lunch', 'NN'), ('in', 'IN'), ('the', 'DT'), ('kitchen', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#sent4 = \"A grey cat was asleep on a rocking chair.\"\n",
    "sent4 = \"mom cooks chicken soup for lunch in the kitchen\"\n",
    "sent4_tokens=nltk.pos_tag(word_tokenize(sent4))\n",
    "print(sent4_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT grey/JJ cat/NN)\n",
      "  was/VBD\n",
      "  asleep/RB\n",
      "  on/IN\n",
      "  (NP a/DT rocking/NN chair/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar5=r\" NP: {<DT><.*><NN>}\"\n",
    "chunk_parser=nltk.RegexpParser(grammar5)\n",
    "\n",
    "chunk5_result=chunk_parser.parse(sent4_tokens)\n",
    "print(chunk5_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Dia/PRP\n",
      "  selalu/RB\n",
      "  keluar/VBP\n",
      "  menggunakan/VBP\n",
      "  jaket/NN\n",
      "  karena/IN\n",
      "  (NP dia/PRP alergi/JJ matahari/VB)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "\"Dia selalu keluar menggunakan jaket, karena dia alergi sinar matahari\"\n",
    "indo_ex = [('Dia','PRP'),('selalu','RB'),('keluar','VBP'),('menggunakan','VBP'),('jaket','NN'),('karena','IN'), ('dia','PRP'),('alergi','JJ'),('matahari','VB'), ('.','.')]\n",
    "grammarindo = r\"NP:{<PRP><JJ><VB>?}\"\n",
    "chunkindo = nltk.RegexpParser(grammarindo)\n",
    "chunkresult = chunkindo.parse(indo_ex)\n",
    "print(chunkresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Ibu/NN memasak/VBZ sup/NN ayam/NN)\n",
      "  untuk/IN\n",
      "  makan siang/NN\n",
      "  di/IN\n",
      "  dapur/NN)\n"
     ]
    }
   ],
   "source": [
    "\"Ibu memasak sup ayam untuk makan siang di dapur\"\n",
    "indo_ex = [('Ibu', 'NN'), ('memasak', 'VBZ'), ('sup', 'NN'), ('ayam', 'NN'), ('untuk', 'IN'), ('makan siang', 'NN'), ('di', 'IN'),('dapur', 'NN')]\n",
    "grammarindo = r\"NP:{<NN><VBZ><NN>*}\"\n",
    "chunkindo = nltk.RegexpParser(grammarindo)\n",
    "chunkresult = chunkindo.parse(indo_ex)\n",
    "print(chunkresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule #8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Ibu memasak sup ayam untuk makan siang di dapur\"\n",
    "indo_ex = [('Ibu', 'NN'), ('memasak', 'VBZ'), ('sup', 'NN'), ('ayam', 'NN'), ('untuk', 'IN'), ('makan siang', 'NN'), ('di', 'IN'),('dapur', 'NN')]\n",
    "grammarindo = r\"NP:{<NN><VBZ><NN>*}\"\n",
    "chunkindo = nltk.RegexpParser(grammarindo)\n",
    "chunkresult = chunkindo.parse(indo_ex)\n",
    "print(chunkresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule #9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"When you’re reading a sentence, you don’t understand it word by word, but rather phrase by phrase. \\n\n",
    "Phrases are groups of words that can be bundled together, and they’re related by the rules of grammar. \\n\n",
    "A noun phrase will include nouns and adjectives, and a verb phrase will include a verb and a noun, for example.  \\n\n",
    "These phrases are the building blocks of language, and we naturally chunk sentences into phrase blocks just as we chunk visual images into objects.\\n\n",
    "\n",
    "It’s now how long a sentence is that makes it hard to understand.  \\n\n",
    "It’s how long you have to wait for a phrase to be completed.\\n\n",
    "What this means is that we don’t treat every word individually as we hear it; \\n\n",
    "we treat words as parts of phrases and have a buffer (a very short-term memory) that stores the words as they come in, \\n\n",
    "until they can be allocated to a phrase. \\n\n",
    "\n",
    "Human languages have the special property of being recombinant. \\n\n",
    "This means a sentence isn’t woven like a scarf, where if you want to add more detail you have to add it at the end. \\n\n",
    "Sentences are more like Lego. The phrases can be broken up and combined with other sentences or popped open in the middle \\n\n",
    "and more bricks added.\\n\n",
    "\n",
    "The way sentences are understood is that they’re parsed into phrases. \\n\n",
    "One type of phrase is a noun phrase, the object of the sentence. \\n\n",
    "In ‘This sentence is a an example,’ the noun phrase is ‘this sentence.’ \\n\n",
    "For the second, it’s ‘this boring sentence.\\n\n",
    "\n",
    "Once a noun phrase is fully assembled, it can be packaged up and properly understood by the rest of the brain. \\n\n",
    "During the time you’re reading the sentence, however, the words sit in your verbal working memory – \\n\n",
    "a kind of short-term buffer – until the phrase is finished.\\n\n",
    "\n",
    "Verb phrases work the same way.  When your brain sees ‘is,’ it knows there’s a verb phrase starting and holds \\n\n",
    "the subsequent words in memory until the phrase has been closed off (with the word ‘example,’ in the first sentence \\n\n",
    "in the previous list). \\n\n",
    "Similarly, the last part of the final sentence, ‘of sentence structure,’ is a prepositional phrase, \\n\n",
    "so it’s also self-contained. \\n\n",
    "Phrase boundaries make sentences much easier to understand. \\n\n",
    "Rather than the object of the third example sentence being three times more complex than the first \\n\n",
    "(it’s three words: ‘long, boring sentence’ versus one, ‘sentence’), it can be understood as the same object, but with modifiers.\\n\n",
    "\n",
    "A sentence takes on a treelike structure, for these simple examples, in which phrases are smaller trees within that. \\n\n",
    "To understand a whole phrase, its individual tree has to join up. \\n\n",
    "These sentences are all easy to understand because they’re composed of very small trees that are completed quickly.\\n\n",
    "\n",
    "To find phrase boundaries, we check individual word meaning and likelihood of word order, \\n\n",
    "continually revise the meaning of the sentence, and so on, all the while the buffer is growing.\\n\n",
    "\n",
    "A characteristic of good speeches (or anything passed down in an oral tradition) is that they minimize the amount of working memory, or buffer, required to understand them. \n",
    "\n",
    "This doesn’t matter so much for written text, in which you can skip back and read the sentence again to figure it out; you have only one chance to hear and comprehend the spoken word, so you’d  better get it right the first time around. \n",
    "\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
